# -*- coding: utf-8 -*-
"""ML_proj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cymzFwz6V-6Q8BJhmBAaEqnuZJIlsWSn

# The project

## Team

Team members          | @polytechnique |
---                   | ---
Bruno Fernandes Iorio | bruno.fernandes-iorio
Adam Neveaux          | to be added
Daniela Cojocaru      | to be added

## Goals

The aim of the project is to design a model for sentiment analysis. We consider a dataset of restaurant reviews (https://www.kaggle.com/datasets/joebeachcapital/restaurant-reviews), with reviews and rating. :


  The final goal that we intend for this model is to attribute a rating for a specific review, possibly for a restaurant, but not limited to it.

#Preliminaries

We start with important imports:
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import string


import sklearn
import sklearn.linear_model
import sklearn.pipeline
import sklearn.preprocessing
import sklearn.metrics as metrics
from sklearn.tree import DecisionTreeClassifier


import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Activation, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.optimizers import SGD


from gensim.corpora import HashDictionary

nltk.download('stopwords')
nltk.download('punkt')

from google.colab import drive
drive.mount('/content/drive')
SW = stopwords.words("english")
m = set(['no', 'nor', 'not',"hadn't","hasn","haven","isn", "shouldn't", "wasn't","weren't","weren","mustn't","wouldn",'too','very','don', "don't",'aren', "aren't",'didn', "didn't","isn't",'few','some','mightn', "mightn't", 'mustn', 'needn', "needn't", 'shan', "shan't", 'shouldn', 'wasn', 'won','more','most',"couldn't","doesn't","doesn","couldn","hadn","hasn't","haven't","havent","won't","wouln't","wouldn't"])
SW = [w for w in SW if w not in m]
print(SW)

"""We get the dataset, and auxiliary datasets/lists/sets for the project"""

data = pd.read_csv("/content/drive/My Drive/Restaurant reviews.csv")

pos_and_neg = pd.read_excel("/content/drive/My Drive/Positive and Negative Word List.xlsx")

neg = pos_and_neg["Negative Sense Word List"].copy() # dataframe with negative words
pos = pos_and_neg["Positive Sense Word List"].copy() # dataframe with positive words
poswords = [pos[i] for i in range(len(pos))]
negwords = [neg[i] for i in range(len(neg))]


negation_words = set(["no",'cant',"can't",'not',"isn't","isnt",'nobody', 'never','neither','nor','barely','hardly','scarcely','seldom','rarely','nothing', 'nor',"hadn't","hasn","haven","isn","shouldn", "shouldnt","shouldn't", "wasn't","wasnt","'t","n't","mustn't","mustn","weren't","weren","mustn't","wouldn",'don', "don't","dont",'aren',"arent", "aren't",'didn', "didnt","didn't","isn't","couldn't","doesn't","doesn","couldn","couldnt","hadn","havent","hasn't","haven't","havent","wasnt","wasn","wont","won't","wouldnt","wouln't","wouldn't"])
# set of words used for negation "not", "no", etc...

pos.dropna(inplace=True,ignore_index=True)
neg.dropna(inplace=True,ignore_index=True)

"""```data ``` contains the restaurant reviews.

```poswords``` is a dataframe containing positive sense words.

```negwords``` is a dataframe containing negative sense words.

```negation_words```is a set containing negation words.

# Cleaning data
"""

data_cleaned = data[['Review', 'Rating']] # just the important features
data_cleaned.dropna(inplace=True,ignore_index=True) # remove nan rows

data_train = data_cleaned[:-40]

data_test = data_cleaned[-40:]
data_test.dropna(inplace=True,ignore_index=True)

y = data_train[["Rating"]]

"""# Data pre-processing

## Preprocessing the Ratings
"""

def makeint(a):
  arr = a.copy()
  for i in range(len(arr)):
    if arr["Rating"][i] == 'Like':
      arr["Rating"][i] = 3
    if type(arr["Rating"][i]) is not int:
      arr["Rating"][i] = int(arr["Rating"][i][0])
    arr["Rating"][i] = int(arr["Rating"][i])
  return arr

"""The function ```makeint()``` corrects a few problems of the dataset, where one of the elements is the string "Like", instead of a number.

## Preprocessing the Reviews
"""

def getmax(X): ## useful for padding with 0's
  maxlen = 0
  for i in X:
    if len(i) > maxlen:
      maxlen = len(i)
  return maxlen

def remove_punc(a: pd.DataFrame) : ## remove punctuation
  df = a.copy()
  df["Review"] = df['Review'].str.replace(r'[^\w\s]','',regex=True)
  return df

def remove_stopwords_from_text(text) : #Remove useless words from teh text#
  tx = text.split()
  new_tx = " ".join([w for w in tx if not w in SW])
  return new_tx

def remove_stopwords(a: pd.DataFrame):
  df = a.copy()
  for i in range(len(df["Review"])):
    df["Review"][i] = remove_stopwords_from_text(df["Review"][i])
  return df

def find_most_freq(df: pd.DataFrame):
  freq = dict()
  for i in range(len(df["Review"])):
    for word in df["Review"][i].split():
      if word not in freq.keys():
        freq[word] = 0
      freq[word] += 1
  sort_freq = dict(sorted(freq.items(), key=lambda item: item[1])[:-1])
  return sort_freq

def remove_freq(a: pd.DataFrame,freq = None):
  df = a.copy()
  if freq is None:
    freq = find_most_freq(df)
  morefreq = list(freq.keys())[-35:]
  lessfreq = [w for w in list(freq.keys()) if freq[w] <= 50]
  for i in range(len(df["Review"])):
    text = df["Review"][i]
    text_list = text.split()
    new_text = " ".join([w for w in text_list if (w not in lessfreq)])
    df["Review"][i] = new_text
  return df


global_freq_dict = None
def preprocessing(a: pd.DataFrame):
  global global_freq_dict
  df = a.copy()
  df["Review"] = df['Review'].str.lower()
  df = remove_punc(df)
  df = remove_stopwords(df)
  if global_freq_dict is None:
    global_freq_dict = find_most_freq(df)
  df = remove_freq(df,global_freq_dict)
  for i in range(len(df)):
    df["Review"][i] = word_tokenize(df["Review"][i])
  return df

"""```getmax(X) ``` :  Returns the length of the biggest list in X. Useful for padding with zeros.

```remove_punc(a)```: Returns a dataframe where the Reviews have no punctuation

```remove_stop_words_from_text(text)```: Returns a text without the stopwords specified in ```SW```.

```remove_stopwords(a)``` Returns a dataframe where the Revuews don't have any of the stopwords specified in ```SW```.

``` find_most_freq(df) ``` : Returns a dictionary with the frequence of each word in the reviews dataset.

```remove_freq(a,freq=None)```: Remove words that are not frequent in the dataset, because they are not so relevant and make the dataset clearer.

``` global_freq_dict ``` : dictionary to be used with the frequences - set to None first , but modified after first iteration. (global variable)

``` preprocessing(a) ``` - apply all the functions above into the reviews and return a preprocessed dataset

## Applying the functions
"""

# dct = HashDictionary(debug=False)

y_new = makeint(y)
X_new = preprocessing(data_train)[["Review"]]


# print(dct.doc2bow([X_new]))

"""Now, ```X_new``` is our preprocessed data. Let's print it out:"""

X_new

"""Let's also print ```y_new```"""

y_new

"""# Feature Extraction"""

data_test_preprocessed = preprocessing(data_test)

"""## Extraction for Linear Regression"""

X_new["features_linear"] = X_new["Review"]
#nltk.download('averaged_perceptron_tagger')


for i in range(len(X_new)):
  text = X_new["Review"][i]
  pos_word_count = sum(1 for word in text if word in poswords)
  neg_word_count = sum(1 for word in text if word in negwords)
 # pos_tags = pos_tag(text)
  #pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)
  features = {

        'pos_word_count': pos_word_count,
        'neg_word_count': neg_word_count,
     #   'num_adjectives': pos_counts['JJ'] + pos_counts['JJR'] + pos_counts['JJS'],
     #   'num_adverbs': pos_counts['RB'] + pos_counts['RBR'] + pos_counts['RBS']
  }
  list_of_features = list(features.values())
  X_new["features_linear"][i] = list_of_features

X_final_Data = X_new.copy()

#def transformNumb(df):
  #X = []
  #maxlen = getmax(df["Review"])
  #for rev in df["Review"]:
    #row = np.zeros(maxlen)
    #for i in range(len(rev)):
      #if  rev[i] in poswords:
       # row[i] = 1
     # elif rev[i] in negwords:
      #  row[i] = -1
    #row.append(row[0]**2)
    #row.append(row[1]**2)

    #X.append(row)
  #return np.array(X)

data_test_preprocessed["features_linear"] = data_test_preprocessed["Review"]
for i in range(len(data_test_preprocessed)):
  text = data_test_preprocessed["Review"][i]
  pos_word_count = sum(1 for word in text if word in poswords)
  neg_word_count = sum(1 for word in text if word in negwords)
  #pos_tags = pos_tag(text)
  #pos_counts = nltk.FreqDist(tag for (word, tag) in pos_tags)
  features = {

        'pos_word_count': pos_word_count,
        'neg_word_count': neg_word_count,
     #   'num_adjectives': pos_counts['JJ'] + pos_counts['JJR'] + pos_counts['JJS'],
     #   'num_adverbs': pos_counts['RB'] + pos_counts['RBR'] + pos_counts['RBS']
  }
  list_of_features = list(features.values())
  data_test_preprocessed["features_linear"][i] = list_of_features

"""## Extraction for the CNN model"""

nltk.download('averaged_perceptron_tagger')

"""With the code cell abov, we will be able to identify grammatical structures inside our texts with the function pos_tag."""

X_new["features_CNN"] = X_new["Review"]
for i in range(len(X_new)):
  text = X_new["Review"][i]
  tags = pos_tag(text)
  text_changed = np.zeros((getmax(X_new["Review"]),4))
  for j in range(len(tags)):
    if tags[j][0] in negation_words:
      text_changed[j,3] = 1
    elif tags[j][0] in poswords:
      text_changed[j,0] = 1
    elif tags[j][0] in negwords:
      text_changed[j,0] = -1

    if tags[j][1] in ["JJ","JJR","JJS"]: #adjectives
      text_changed[j,1] = 1
    if tags[j][1] in ["RB","RBR","RBS"]: #adverbs
      text_changed[j,2] = 1

  X_new["features_CNN"][i] = text_changed
X_new

data_test_preprocessed["features_CNN"] = data_test_preprocessed["Review"]
for i in range(len(data_test_preprocessed)):
  text = data_test_preprocessed["Review"][i]
  tags = pos_tag(text)
  text_changed = np.zeros((getmax(X_new["Review"]),4))
  for j in range(len(tags)):
    if tags[j][0] in negation_words:
      text_changed[j,3] = 1
    elif tags[j][0] in poswords:
      text_changed[j,0] = 1
    elif tags[j][0] in negwords:
      text_changed[j,0] = -1

    if tags[j][1] in ["JJ","JJR","JJS"]: #adjectives
      text_changed[j,1] = 1
    if tags[j][1] in ["RB","RBR","RBS"]: #adverbs
      text_changed[j,2] = 1

  data_test_preprocessed["features_CNN"][i] = text_changed
data_test_preprocessed

for i in range(10):
  print(pos_tag(X_new["Review"][9900])[i],X_new["features_CNN"][9900][i])
X_CNN = np.array([X_new["features_CNN"][i] for i in range(len(X_new["features_CNN"]))])
print(np.shape(X_CNN[0]))

"""#The model

## Linear Regression model

This is our simplest model. I just takes into consideration the number of positive and negative words of a given text, and apply a regression on this. We justify this approach with the following plot:
"""

X_regression = np.array([X_new["features_linear"][i]for i in range(len(X_new))])
y_map = [float(y_new["Rating"][i]) for i in range(len(y_new["Rating"]))]
## plots
plt.scatter(x=X_regression[:,0],y=X_regression[:,1],c=y_map,cmap="rainbow")
plt.xlabel("Number of positive words")
plt.ylabel("Number of negative words")
plt.colorbar(label="Rating")
plt.show()

"""As we can see from the plot, the more positve words and less negative words, the higher the rating in average. We can, then, approximate the distribution linearly in the 3d space. But still it is very hard to cluster the in between scores."""

def linear_Regression_model():
  model = sklearn.linear_model.LinearRegression()
  return model

model_regression = linear_Regression_model()
model_regression.fit(X_regression,y_new)

test_linear = np.array([data_test_preprocessed["features_linear"][i] for i in range(len(data_test_preprocessed))])
pred = model_regression.predict(test_linear)
for i in range(len(test_linear)):
  print(i)
  print("predicted: ", pred[i])
  print("actual: ", data_test_preprocessed["Rating"][i])

"""## CNN model"""

def CNN_model(input_shape):
  model = tf.keras.Sequential()

  model.add(Conv2D(filters=10,kernel_size=(4,4),padding='valid',activation="relu", input_shape=input_shape))
  model.add(Conv2D(filters=10,kernel_size=(4,1),padding='valid',activation="relu", input_shape=input_shape))

  model.add(MaxPooling2D(pool_size=(4,1)))
  model.add(Dropout(rate=0.25))
  model.add(BatchNormalization())


  model.add(Conv2D(filters=25,kernel_size=(4,1),padding='valid',activation="relu", input_shape=input_shape))
  model.add(Conv2D(filters=25,kernel_size=(4,1),padding='valid',activation="relu", input_shape=input_shape))

  model.add(MaxPooling2D(pool_size=(4,1)))
  model.add(Dropout(rate=0.25))
  model.add(BatchNormalization())

  model.add(Conv2D(filters=50,kernel_size=(4,1),padding='valid',activation="relu", input_shape=input_shape))
  model.add(Conv2D(filters=50,kernel_size=(4,1),padding='valid',activation="relu", input_shape=input_shape))


  model.add(MaxPooling2D(pool_size=(2,1)))
  model.add(Dropout(rate=0.25))
  model.add(BatchNormalization())

  model.add(Flatten())

  model.add(Dense(512, activation='relu'))

  model.add(Dense(256, activation='relu'))

  model.add(Dense(128, activation='relu'))



  model.add(Dense(5,activation='softmax'))
  model.compile(loss='categorical_crossentropy',
    optimizer='SGD',
    metrics=['accuracy'])
  return model

model_cnn = CNN_model((389,4,1))
model_cnn.summary()

y_map = np.array(y_map)
z = y_map.copy()
for i in range(len(y_map)):
  z[i] = int(y_map[i])-1

y_CNN = keras.utils.to_categorical(z, num_classes=5)
model_cnn.fit(X_CNN,y_CNN,epochs=10)

to_predict_cnn = np.array([data_test_preprocessed["features_CNN"][i] for i in range(len(data_test_preprocessed["features_CNN"]))])

prediction_CNN = model_cnn.predict(to_predict_cnn)
print(data_test["Review"][23])
pred = []
for guess in prediction_CNN:
  for j in range(len(guess)):
    if guess[j] == max(guess):
      guess[j]=1
      pred.append(j)
    else:
      guess[j]=0
for i in range(len(data_test_preprocessed["Rating"])):
  if type(data_test_preprocessed["Rating"][i]) is str:
    data_test_preprocessed["Rating"][i] = data_test_preprocessed["Rating"][i][0]
  data_test_preprocessed["Rating"][i] = int(data_test_preprocessed["Rating"][i])
data_test_preprocessed["Rating"][i]
t = 0
for i in range(len(pred)):
  print(str(i) + ") predicted and real rating: ", pred[i]+1, int(data_test_preprocessed["Rating"][i]), abs(pred[i] +1 - int(data_test_preprocessed["Rating"][i])) )
  if abs(pred[i] +1 - int(data_test_preprocessed["Rating"][i])) <= 2:
    t += 1
print(t)

##Positive-Negative word list separation, into Decision Tree

##ALL SORTA decision tree ideas getting tested here:

#dtree = DecisionTreeClassifier(criterion="log_loss", max_leaves=15)

#bruno, I tried bayes for over three hours,

# def based_bayes_model_fit(X, y, verbose=True):



# def based_bayes_model_predict(X, weights):
#   return pass